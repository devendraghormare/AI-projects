
# NL2SQL Assistant

**Convert Natural Language Questions to SQL Queries** and execute them on your database using LLMs like OpenAI's GPT.

---

## Features

* ğŸ” Converts user questions to SQL using GPT-4o
* ğŸ“Š Executes SQL queries on a live Postgres
* ğŸ“‚ Extracts schema context dynamically (with caching)
* âš¡ Caches LLM outputs and query results for speed
* ğŸ” Validates SQL for safety (prevents risky modifications)
* ğŸ“ˆ Logs token usage, queries, and performance

---

## ğŸ—ï¸ Project Structure


nl2sql-assistant/

â”œâ”€â”€ app/                        # FastAPI Backend
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â””â”€â”€ endpoints/
â”‚   â”‚           â””â”€â”€ query.py         # Endpoints for /generate-sql and /run-sql
â”‚   â”œâ”€â”€ core/                        # App settings and logger
â”‚   â”œâ”€â”€ db/                          # DB session and schema reflection
â”‚   â”œâ”€â”€ llm/                         # Prompt templates and OpenAI calls
â”‚   â”œâ”€â”€ models/                      # Pydantic schemas
â”‚   â”œâ”€â”€ services/                    # SQL validation and execution logic
â”‚   â”œâ”€â”€ utils/                       # Helpers for formatting, caching, etc.
â”‚   â””â”€â”€ main.py                      # FastAPI app entrypoint
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ streamlit_app.py             # Streamlit UI
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup_db.py                  # Initial DB setup
â”‚
â”œâ”€â”€ tests/                           # Unit and integration tests
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml          # Combined stack setup
â”‚
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .env                             # Environment variables (API keys, DB URL)
â””â”€â”€ README.md                        # Project documentation

---

## âš™ï¸ Tech Stack

* **Backend** : FastAPI, SQLAlchemy (async)
* **LLM** : OpenAI GPT-4o
* **Cache** : diskcache (local), Redis (optional for prod)
* **Database** : PostgreSQL 
* **Container** : Docker + Docker Compose
* **Extras** : Loguru,Pydantic, Uvicorn

---

## ğŸ“¦ Installation

### 1. Clone the Repo

https://github.com/devendraghormare/AI-projects.git

### 2. Set up `.env`

<code class="whitespace-pre! language-env"><span>OPENAI_API_KEY=your-openai-api-key
DATABASE_URL=postgresql+asyncpg://user:pass@localhost/dbname
</span></code></div></div></pre>

### 3. Install Dependencies

<code class="whitespace-pre! language-bash"><span><span>pip install -r requirements.txt
</span></span></code></div></div></pre>

### 4. Run the Server

cd app

uvicorn main:app --reload

---

## ğŸ³ Docker Deployment

<code class="whitespace-pre! language-bash"><span><span>docker-compose up --build
</span></span></code></div></div></pre>

Make sure your `.env` is configured. Example `docker-compose.yml` supports:

* FastAPI app
* PostgreSQL with volume
* Redis (optional for caching)

---

## ğŸ“¬ API Endpoints

* `POST /v1/query`: Accepts natural language and returns SQL + results

### Example Request

<code class="whitespace-pre! language-json"><span><span>{</span><span>
  </span><span>"question"</span><span>:</span><span></span><span>"Show me the top 5 products by sales"</span><span>,</span><span>
  </span><span>"allow_modifications"</span><span>:</span><span></span><span>false</span><span>
</span><span>}</span><span>
</span></span></code></div></div></pre>

---

## ğŸ“ˆ Logging & Caching

* **Diskcache** stores:
  * Schema for 1 hour
  * LLM outputs for 10 minutes
  * SELECT query results for 5 minutes
* Logs are written using Loguru with daily rotation.
